{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc1cdda2f082405799bdddc139f8c7d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_758ad3a24a0b47aa973a3a9d4e898642",
              "IPY_MODEL_d1f4b685c1ca4e1d8ffc2cebb987e3ec",
              "IPY_MODEL_773387b0e3e4470a8de630c5df697930"
            ],
            "layout": "IPY_MODEL_60734a9f2dcc4d8c9ffbeae88de95f5c"
          }
        },
        "758ad3a24a0b47aa973a3a9d4e898642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e084a4121a343598c2ac0ea341e1ded",
            "placeholder": "​",
            "style": "IPY_MODEL_c9b6dde1f30744c69820daeb38379ed6",
            "value": "Processing files: 100%"
          }
        },
        "d1f4b685c1ca4e1d8ffc2cebb987e3ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_630455a5ace748a7aee75a0e0ab4e97b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_346fe18071c64333a9ed51231e4effe1",
            "value": 1
          }
        },
        "773387b0e3e4470a8de630c5df697930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68377cb0dec74cd7b922efc084d87825",
            "placeholder": "​",
            "style": "IPY_MODEL_c1c0c3824fcc4dd3af225aa61338f58c",
            "value": " 1/1 [00:02&lt;00:00,  2.23s/it]"
          }
        },
        "60734a9f2dcc4d8c9ffbeae88de95f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e084a4121a343598c2ac0ea341e1ded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9b6dde1f30744c69820daeb38379ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "630455a5ace748a7aee75a0e0ab4e97b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "346fe18071c64333a9ed51231e4effe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68377cb0dec74cd7b922efc084d87825": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c0c3824fcc4dd3af225aa61338f58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook allows to generate output with the trained autoencoder model, and some data that can be audio generated by MusicGen. Be careful you need to specify the path to the model as well as the path to the data."
      ],
      "metadata": {
        "id": "sEP9zUhoxK6a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5tKdCoaYrvDW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import os\n",
        "import glob\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the model"
      ],
      "metadata": {
        "id": "4ZwgJjCbwSM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleConvAutoencoder, self).__init__()\n",
        "        self.enc_conv1 = nn.Sequential(nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU())\n",
        "        self.enc_conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), nn.ReLU())\n",
        "        self.enc_conv3 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), nn.ReLU())\n",
        "        self.dec_conv1 = nn.Sequential(nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1), nn.ReLU())\n",
        "        self.dec_conv2 = nn.Sequential(nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1), nn.ReLU())\n",
        "        self.dec_conv3 = nn.Sequential(nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.enc_conv1(x)\n",
        "        x2 = self.enc_conv2(x1)\n",
        "        x3 = self.enc_conv3(x2)\n",
        "        x4 = self.dec_conv1(x3)\n",
        "        x4 = self.crop(x4, x2.shape)\n",
        "        x5 = self.dec_conv2(x4)\n",
        "        x5 = self.crop(x5, x1.shape)\n",
        "        x6 = self.dec_conv3(x5)\n",
        "        output = self.crop(x6, x.shape)\n",
        "        return output\n",
        "\n",
        "    def crop(self, tensor_to_crop, target_shape):\n",
        "        target_height, target_width = target_shape[2], target_shape[3]\n",
        "        current_height, current_width = tensor_to_crop.shape[2], tensor_to_crop.shape[3]\n",
        "        delta_h = current_height - target_height\n",
        "        delta_w = current_width - target_width\n",
        "        h_start = delta_h // 2\n",
        "        w_start = delta_w // 2\n",
        "        return tensor_to_crop[:, :, h_start : h_start + target_height, w_start : w_start + target_width]\n",
        "\n",
        "def process_file_to_tensor(file_path, device, n_fft=1024):\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    waveform = waveform.to(device)\n",
        "    if sr != 48000:\n",
        "        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=48000)(waveform)\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "    window = torch.hann_window(n_fft, device=device)\n",
        "    stft = torch.stft(waveform, n_fft=n_fft, hop_length=n_fft//4, window=window, return_complex=True)\n",
        "    magnitude = torch.abs(stft)\n",
        "    phase = torch.angle(stft)\n",
        "    magnitude_norm = (magnitude - magnitude.min()) / (magnitude.max() - magnitude.min() + 1e-8)\n",
        "    magnitude_norm_3d = magnitude_norm.unsqueeze(0)\n",
        "    return magnitude_norm_3d, phase"
      ],
      "metadata": {
        "id": "qw9ax12mrvj5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load trained model"
      ],
      "metadata": {
        "id": "xicHMK0Kwc29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Load the trained model\n",
        "model_path = \"trained_autoencoder_simple.pth\"  # Change this to your model path\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = SimpleConvAutoencoder().to(device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "print(f\"Loaded model from {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgnBGMkorvp9",
        "outputId": "62e551c3-9310-4a53-d1dd-eef064f0c451"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded model from /content/drive/MyDrive/finetuned_autoencoder_simple.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "S7jORRp1wjeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be audio generated with MusicGen"
      ],
      "metadata": {
        "id": "xqO96lKbwrJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_audio_files(directory, extension=\".wav\"):\n",
        "    return sorted(glob.glob(os.path.join(directory, f\"*{extension}\")))\n",
        "\n",
        "\n",
        "audio_dir = \"MusicGen_data\"  # Change this to your data directory\n",
        "audio_files = list_audio_files(audio_dir, extension=\".wav\")\n",
        "print(f\"Found {len(audio_files)} audio files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "578m50lYrvs4",
        "outputId": "c460856f-f83e-4cbf-b636-605c5c92689c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 43 audio files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get output of data through trained model"
      ],
      "metadata": {
        "id": "11KkCvlRw6kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_outputs_on_files(model, file_list, device, preprocess_fn, max_files=None):\n",
        "    \"\"\"\n",
        "    Applies the model to a list of audio files and returns the outputs.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Trained autoencoder model.\n",
        "        file_list (list): List of audio file paths.\n",
        "        device (str or torch.device): Device to use.\n",
        "        preprocess_fn (callable): Function to preprocess audio file to model input.\n",
        "        max_files (int, optional): If set, only process up to this many files.\n",
        "\n",
        "    Returns:\n",
        "        outputs (list): List of model outputs (tensors).\n",
        "        inputs (list): List of input tensors (for reference).\n",
        "        file_names (list): List of file names processed.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    inputs = []\n",
        "    file_names = []\n",
        "    with torch.no_grad():\n",
        "        for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing files\")):\n",
        "            if max_files is not None and idx >= max_files:\n",
        "                break\n",
        "            input_tensor, _ = preprocess_fn(file_path, device)\n",
        "            if input_tensor.dim() == 4:\n",
        "                input_tensor = input_tensor.squeeze(0)\n",
        "            input_tensor = input_tensor.unsqueeze(0).to(device)  # Add batch dimension\n",
        "            output = model(input_tensor)\n",
        "            outputs.append(output.cpu())\n",
        "            inputs.append(input_tensor.cpu())\n",
        "            file_names.append(file_path)\n",
        "    return outputs, inputs, file_names"
      ],
      "metadata": {
        "id": "iQAFIaqMrvv0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, inputs, processed_files = get_model_outputs_on_files(\n",
        "    model, [first_audio_file], device, process_file_to_tensor, max_files=5\n",
        ")\n",
        "print(f\"Processed {len(outputs)} files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "dc1cdda2f082405799bdddc139f8c7d5",
            "758ad3a24a0b47aa973a3a9d4e898642",
            "d1f4b685c1ca4e1d8ffc2cebb987e3ec",
            "773387b0e3e4470a8de630c5df697930",
            "60734a9f2dcc4d8c9ffbeae88de95f5c",
            "1e084a4121a343598c2ac0ea341e1ded",
            "c9b6dde1f30744c69820daeb38379ed6",
            "630455a5ace748a7aee75a0e0ab4e97b",
            "346fe18071c64333a9ed51231e4effe1",
            "68377cb0dec74cd7b922efc084d87825",
            "c1c0c3824fcc4dd3af225aa61338f58c"
          ]
        },
        "id": "hidTMzCkrvy1",
        "outputId": "813072be-e2e6-4f26-8a2f-b6c3c69ba500"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc1cdda2f082405799bdddc139f8c7d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1 files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconstruct the audio files"
      ],
      "metadata": {
        "id": "JfAa2XUexAD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_audio(output_magnitude, original_phase, n_fft=1024):\n",
        "    \"\"\"\n",
        "    Converts the model's output magnitude and original phase back to a waveform.\n",
        "\n",
        "    Args:\n",
        "        output_magnitude (Tensor): The output magnitude tensor from the model (shape: [1, freq, time] or [batch, 1, freq, time]).\n",
        "        original_phase (Tensor): The original phase tensor (shape: [1, freq, time] or [freq, time]).\n",
        "        n_fft (int): FFT window size.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: The reconstructed waveform, normalized.\n",
        "    \"\"\"\n",
        "    # If output_magnitude has batch and channel dimensions, squeeze them\n",
        "    if output_magnitude.dim() == 4:\n",
        "        output_magnitude = output_magnitude.squeeze(0).squeeze(0)\n",
        "    elif output_magnitude.dim() == 3:\n",
        "        output_magnitude = output_magnitude.squeeze(0)\n",
        "\n",
        "    # Resize output magnitude to match the original phase shape\n",
        "    target_shape = original_phase.shape\n",
        "    output_magnitude_resized = F.interpolate(\n",
        "        output_magnitude.unsqueeze(0).unsqueeze(0),  # add batch and channel dims\n",
        "        size=(target_shape[-2], target_shape[-1]),\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    ).squeeze(0).squeeze(0)\n",
        "\n",
        "    # Combine magnitude and phase to get the complex spectrogram\n",
        "    spectrogram_complex = torch.polar(output_magnitude_resized, original_phase)\n",
        "\n",
        "    # Inverse STFT to get waveform\n",
        "    waveform = torch.istft(spectrogram_complex, n_fft=n_fft, hop_length=n_fft//4)\n",
        "\n",
        "    # Normalize waveform\n",
        "    waveform = waveform / (waveform.abs().max() + 1e-8)\n",
        "    return waveform"
      ],
      "metadata": {
        "id": "QTx9bBwOvXRo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for output_tensor, file_path in zip(outputs, processed_files):\n",
        "    # Get the original phase\n",
        "    _, phase = process_file_to_tensor(file_path, device)\n",
        "    # Reconstruct the audio waveform\n",
        "    waveform = postprocess_audio(output_tensor, phase)\n",
        "    # Build the output filename\n",
        "    base_name = os.path.basename(file_path)\n",
        "    output_filename = f\"reconstructed_{base_name}\"\n",
        "    # Save the waveform as a .wav file\n",
        "    torchaudio.save(output_filename, waveform.cpu(), 48000)\n",
        "    print(f\"Saved reconstructed audio as {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esCY3P8cvzKF",
        "outputId": "fe2f371f-af9c-4eb1-eca1-6281dceccf4d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved reconstructed audio as reconstructed_A_Balkan_brass_band_with_high_energy_and.wav\n"
          ]
        }
      ]
    }
  ]
}