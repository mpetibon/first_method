{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook defines a simple model of autoencoder type, trains it on the train part of a dataset of pairs of clean and degraded audio, and test the result on the test part of the dataset. Be careful to run this code you need a path pointint to the clean data and one pointing to the degraded data."
      ],
      "metadata": {
        "id": "wNfv3b49xgLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTZlMjt6TnVI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import shutil\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining model"
      ],
      "metadata": {
        "id": "WscdjedQro9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleConvAutoencoder, self).__init__()\n",
        "\n",
        "        # --- ENCODER ---\n",
        "        # Each layer reduces the spatial dimension by 2\n",
        "        self.enc_conv1 = nn.Sequential(nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU())\n",
        "        self.enc_conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), nn.ReLU())\n",
        "        self.enc_conv3 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), nn.ReLU())\n",
        "\n",
        "        # --- DECODER ---\n",
        "        # Each layer increases the spatial dimension by 2\n",
        "        self.dec_conv1 = nn.Sequential(nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1), nn.ReLU())\n",
        "        self.dec_conv2 = nn.Sequential(nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1), nn.ReLU())\n",
        "        self.dec_conv3 = nn.Sequential(nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through Encoder\n",
        "        x1 = self.enc_conv1(x)\n",
        "        x2 = self.enc_conv2(x1)\n",
        "        x3 = self.enc_conv3(x2)\n",
        "\n",
        "        # Pass through Decoder with cropping\n",
        "        x4 = self.dec_conv1(x3)\n",
        "        x4 = self.crop(x4, x2.shape)\n",
        "\n",
        "        x5 = self.dec_conv2(x4)\n",
        "        x5 = self.crop(x5, x1.shape)\n",
        "\n",
        "        x6 = self.dec_conv3(x5)\n",
        "        output = self.crop(x6, x.shape)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def crop(self, tensor_to_crop, target_shape):\n",
        "        \"\"\"\n",
        "        Crop a tensor to match the spatial dimensions of a target shape.\n",
        "        Args:\n",
        "            tensor_to_crop (Tensor): The tensor to be cropped.\n",
        "            target_shape (tuple): The target shape to crop to (should be a 4D shape: [batch, channel, height, width]).\n",
        "        Returns:\n",
        "            Tensor: The cropped tensor.\n",
        "        \"\"\"\n",
        "        target_height, target_width = target_shape[2], target_shape[3]\n",
        "        current_height, current_width = tensor_to_crop.shape[2], tensor_to_crop.shape[3]\n",
        "\n",
        "        delta_h = current_height - target_height\n",
        "        delta_w = current_width - target_width\n",
        "\n",
        "        h_start = delta_h // 2\n",
        "        w_start = delta_w // 2\n",
        "\n",
        "        return tensor_to_crop[:, :, h_start : h_start + target_height, w_start : w_start + target_width]\n",
        "\n",
        "\n",
        "def process_file_to_tensor(file_path, device, n_fft=1024):\n",
        "    \"\"\"\n",
        "    Loads an audio file and converts it to a normalized 3D magnitude spectrogram tensor.\n",
        "    Args:\n",
        "        file_path (str): Path to the audio file.\n",
        "        device (torch.device): Device to load the tensor onto.\n",
        "        n_fft (int): FFT window size.\n",
        "    Returns:\n",
        "        Tuple[Tensor, Tensor]: Normalized magnitude spectrogram (3D tensor), phase tensor.\n",
        "    \"\"\"\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    waveform = waveform.to(device)\n",
        "\n",
        "    if sr != 48000:\n",
        "        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=48000)(waveform)\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    window = torch.hann_window(n_fft, device=device)\n",
        "    stft = torch.stft(waveform, n_fft=n_fft, hop_length=n_fft//4,\n",
        "                      window=window, return_complex=True)\n",
        "\n",
        "    magnitude = torch.abs(stft)\n",
        "    phase = torch.angle(stft)\n",
        "\n",
        "    magnitude_norm = (magnitude - magnitude.min()) / (magnitude.max() - magnitude.min())\n",
        "\n",
        "    magnitude_norm_3d = magnitude_norm.unsqueeze(0)\n",
        "\n",
        "    return magnitude_norm_3d, phase\n",
        "\n",
        "\n",
        "def tensor_to_audio_file(output_magnitude, original_phase, file_path, n_fft=1024):\n",
        "    \"\"\"\n",
        "    Reconstructs and saves an audio file from magnitude and phase tensors.\n",
        "    Args:\n",
        "        output_magnitude (Tensor): The output magnitude tensor from the model.\n",
        "        original_phase (Tensor): The original phase tensor.\n",
        "        file_path (str): Path to save the reconstructed audio file.\n",
        "        n_fft (int): FFT window size.\n",
        "    \"\"\"\n",
        "    target_shape = original_phase.shape\n",
        "    resized_output_magnitude = F.interpolate(output_magnitude, size=(target_shape[1], target_shape[2]), mode='bilinear', align_corners=False)\n",
        "\n",
        "    resized_output_magnitude_squeezed = resized_output_magnitude.squeeze(1)\n",
        "\n",
        "    spectrogram_complex = torch.polar(resized_output_magnitude_squeezed, original_phase)\n",
        "    waveform = torch.istft(spectrogram_complex, n_fft=n_fft, hop_length=n_fft//4)\n",
        "    torchaudio.save(file_path, waveform.cpu(), 48000)\n",
        "\n",
        "def preprocess_audio(file_path):\n",
        "    \"\"\"\n",
        "    Preprocesses an audio file into a normalized magnitude spectrogram and phase.\n",
        "    Args:\n",
        "        file_path (str): Path to the audio file.\n",
        "    Returns:\n",
        "        Tuple[Tensor, Tensor]: Normalized magnitude spectrogram, phase tensor.\n",
        "    \"\"\"\n",
        "    return process_file_to_tensor(file_path, device)\n",
        "\n",
        "def postprocess_audio(output_magnitude, original_phase, n_fft=1024):\n",
        "    \"\"\"\n",
        "    Converts the model's output magnitude and original phase back to a waveform.\n",
        "    Args:\n",
        "        output_magnitude (Tensor): The output magnitude tensor from the model.\n",
        "        original_phase (Tensor): The original phase tensor.\n",
        "        n_fft (int): FFT window size.\n",
        "    Returns:\n",
        "        Tensor: The reconstructed waveform, normalized.\n",
        "    \"\"\"\n",
        "    target_shape = original_phase.shape\n",
        "    resized_output_magnitude = F.interpolate(\n",
        "        output_magnitude,\n",
        "        size=(target_shape[1], target_shape[2]),\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    )\n",
        "\n",
        "    resized_output_magnitude_squeezed = resized_output_magnitude.squeeze(1)\n",
        "\n",
        "    spectrogram_complex = torch.polar(resized_output_magnitude_squeezed, original_phase)\n",
        "    waveform = torch.istft(spectrogram_complex, n_fft=n_fft, hop_length=n_fft//4)\n",
        "\n",
        "    waveform = waveform / (waveform.abs().max() + 1e-8)\n",
        "    return waveform\n"
      ],
      "metadata": {
        "id": "UPKmrUeQToj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing data"
      ],
      "metadata": {
        "id": "fJaeAl9JgQIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this part you need to have a set of clean audio and a set of associated degraded audio saved somewhere"
      ],
      "metadata": {
        "id": "aevRACk0gTE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_audio_pairs(clean_dir, degraded_dir, local_clean_dir=None, max_pairs=1000, file_extension='.mp3'):\n",
        "    \"\"\"\n",
        "    Prepares pairs of degraded and clean audio files for training.\n",
        "\n",
        "    Args:\n",
        "        clean_dir (str): Path to the directory containing clean (high-quality) audio files.\n",
        "        degraded_dir (str): Path to the directory containing degraded audio files.\n",
        "        local_clean_dir (str, optional): If provided, the clean dataset will be copied to this local directory\n",
        "                                         if it does not already exist. If None, no copying is performed.\n",
        "        max_pairs (int): Maximum number of pairs to return (for faster training/testing).\n",
        "        file_extension (str): File extension to look for (default: '.mp3').\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, str]]: List of tuples, each containing (degraded_file_path, clean_file_path).\n",
        "    \"\"\"\n",
        "    # Optionally copy the clean dataset to a local directory\n",
        "    if local_clean_dir is not None:\n",
        "        if os.path.exists(clean_dir):\n",
        "            print(f\"Copying clean dataset from {clean_dir} to {local_clean_dir} using shutil.copytree...\")\n",
        "            if not os.path.exists(local_clean_dir):\n",
        "                shutil.copytree(clean_dir, local_clean_dir, dirs_exist_ok=True)\n",
        "            else:\n",
        "                print(\"Clean dataset already exists in the local directory.\")\n",
        "            clean_dir_to_use = local_clean_dir\n",
        "        else:\n",
        "            print(f\"WARNING: Clean dataset folder not found at {clean_dir}\")\n",
        "            return []\n",
        "    else:\n",
        "        clean_dir_to_use = clean_dir\n",
        "\n",
        "    # Check that both directories exist\n",
        "    if not (os.path.exists(degraded_dir) and os.path.exists(clean_dir_to_use)):\n",
        "        print(\"ERROR: Could not find the degraded or clean dataset directories.\")\n",
        "        return []\n",
        "\n",
        "    # Find matching pairs\n",
        "    data_pairs = []\n",
        "    for root, _, files in os.walk(degraded_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(file_extension):\n",
        "                degraded_path = os.path.join(root, file)\n",
        "                relative_path = os.path.relpath(degraded_path, degraded_dir)\n",
        "                clean_path = os.path.join(clean_dir_to_use, relative_path)\n",
        "\n",
        "                if os.path.exists(clean_path):\n",
        "                    data_pairs.append((degraded_path, clean_path))\n",
        "\n",
        "    print(f\"Found {len(data_pairs)} matching pairs of audio files for training.\")\n",
        "\n",
        "    # Subsample for faster training if needed\n",
        "    training_pairs = data_pairs[:max_pairs]\n",
        "    print(f\"Using a subset of {len(training_pairs)} pairs for this training session.\")\n",
        "\n",
        "    return training_pairs"
      ],
      "metadata": {
        "id": "zxinET32Tom9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioSuperResDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for audio super-resolution tasks.\n",
        "    Each item is a tuple of (degraded_spectrogram, clean_spectrogram).\n",
        "    \"\"\"\n",
        "    def __init__(self, data_pairs, preprocessor, target_length=5000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_pairs (List[Tuple[str, str]]): List of (degraded_path, clean_path) pairs.\n",
        "            preprocessor (callable): Function to process an audio file path into a spectrogram tensor.\n",
        "            target_length (int): The fixed temporal length for all spectrograms.\n",
        "        \"\"\"\n",
        "        self.data_pairs = data_pairs\n",
        "        self.preprocessor = preprocessor\n",
        "        self.target_length = target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        degraded_path, clean_path = self.data_pairs[idx]\n",
        "\n",
        "        # Preprocess both degraded and clean audio files\n",
        "        input_magnitude, _ = self.preprocessor(degraded_path)\n",
        "        target_magnitude, _ = self.preprocessor(clean_path)\n",
        "\n",
        "        # Ensure correct dimensions (C, F, T)\n",
        "        if input_magnitude.dim() == 4:\n",
        "            input_magnitude = input_magnitude.squeeze(0)\n",
        "        if target_magnitude.dim() == 4:\n",
        "            target_magnitude = target_magnitude.squeeze(0)\n",
        "\n",
        "        # Fix the temporal length of both spectrograms\n",
        "        input_magnitude = self.fix_length(input_magnitude, self.target_length)\n",
        "        target_magnitude = self.fix_length(target_magnitude, self.target_length)\n",
        "\n",
        "        return input_magnitude, target_magnitude\n",
        "\n",
        "    def fix_length(self, spectrogram, target_length):\n",
        "        \"\"\"\n",
        "        Adjusts the temporal length of a spectrogram to a fixed size.\n",
        "        Pads with zeros or truncates as needed.\n",
        "\n",
        "        Args:\n",
        "            spectrogram (Tensor): Input spectrogram tensor.\n",
        "            target_length (int): Desired temporal length.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Spectrogram with fixed temporal length.\n",
        "        \"\"\"\n",
        "        current_length = spectrogram.shape[-1]\n",
        "\n",
        "        if current_length > target_length:\n",
        "            return spectrogram[..., :target_length]\n",
        "        elif current_length < target_length:\n",
        "            padding = target_length - current_length\n",
        "            return F.pad(spectrogram, (0, padding), mode='constant', value=0)\n",
        "        else:\n",
        "            return spectrogram\n",
        "\n",
        "# Example: Splitting data into train and test sets\n",
        "# (Assume data_pairs is already defined, e.g., from prepare_audio_pairs)\n",
        "train_pairs = data_pairs[:1000]\n",
        "test_pairs = data_pairs[1000:1200]\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = AudioSuperResDataset(train_pairs, preprocess_audio)\n",
        "test_dataset = AudioSuperResDataset(test_pairs, preprocess_audio)\n",
        "\n",
        "# Create DataLoader objects for batching and shuffling\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(\"PyTorch Dataset and DataLoader are ready.\")"
      ],
      "metadata": {
        "id": "r161jTOKT01I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "eGxfwnpRgsJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_autoencoder(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    num_epochs=5,\n",
        "    learning_rate=1e-4,\n",
        "    criterion=None,\n",
        "    device=None,\n",
        "    model_save_path=\"finetuned_autoencoder_simple.pth\",\n",
        "    print_every=20\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains an autoencoder model on the provided data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The autoencoder model to train.\n",
        "        train_dataloader (DataLoader): DataLoader for the training data.\n",
        "        num_epochs (int): Number of epochs to train for.\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "        criterion (callable, optional): Loss function. If None, uses nn.MSELoss().\n",
        "        device (str or torch.device, optional): Device to use ('cuda', 'cpu', etc). If None, auto-detects.\n",
        "        model_save_path (str): Path to save the trained model weights.\n",
        "        print_every (int): Print loss every N batches.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The trained model.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    if criterion is None:\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n--- Starting Epoch {epoch+1}/{num_epochs} ---\")\n",
        "        for i, (input_batch, target_batch) in enumerate(tqdm(train_dataloader)):\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_batch)\n",
        "            loss = criterion(outputs, target_batch)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % print_every == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    print(\"\\n--- Training Finished! ---\")\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "c6ZaTYaVT04J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our project we did the training with num_epochs = 5, learning_rate = 1e-4 and criterion = nn.MSELoss()."
      ],
      "metadata": {
        "id": "MCa9PI_pmg2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the model on test part of dataset"
      ],
      "metadata": {
        "id": "_YUbBGP4g0j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_and_save_examples(\n",
        "    model,\n",
        "    test_dataloader,\n",
        "    test_pairs,\n",
        "    process_file_to_tensor,\n",
        "    postprocess_audio,\n",
        "    device=None,\n",
        "    criterion=None,\n",
        "    num_examples_to_save=3,\n",
        "    output_dir=\".\",\n",
        "    sample_rate=48000,\n",
        "    print_progress=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluates a model on the test set and saves a few output audio examples.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained model to evaluate.\n",
        "        test_dataloader (DataLoader): DataLoader for the test data.\n",
        "        test_pairs (list): List of (degraded_path, clean_path) pairs, used for file naming.\n",
        "        process_file_to_tensor (callable): Function to process a file path into (magnitude, phase).\n",
        "        postprocess_audio (callable): Function to convert model output and phase into waveform.\n",
        "        device (str or torch.device, optional): Device to use. If None, auto-detects.\n",
        "        criterion (callable, optional): Loss function. If None, uses nn.MSELoss().\n",
        "        num_examples_to_save (int): Number of output examples to save.\n",
        "        output_dir (str): Directory to save output audio files.\n",
        "        sample_rate (int): Sample rate for saving audio.\n",
        "        print_progress (bool): Whether to print progress with tqdm.\n",
        "\n",
        "    Returns:\n",
        "        float: Average test loss.\n",
        "        list: List of saved output filenames.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if criterion is None:\n",
        "        criterion = torch.nn.MSELoss()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    num_examples = 0\n",
        "    num_saved = 0\n",
        "    saved_filenames = []\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        data_iter = tqdm(test_dataloader, desc=\"Test Progress\") if print_progress else test_dataloader\n",
        "        for i, (input_batch, target_batch) in enumerate(data_iter):\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "            outputs = model(input_batch)\n",
        "\n",
        "            loss = criterion(outputs, target_batch)\n",
        "            batch_size = input_batch.size(0)\n",
        "            test_loss += loss.item() * batch_size\n",
        "            num_examples += batch_size\n",
        "\n",
        "            # Save up to num_examples_to_save output examples\n",
        "            if num_saved < num_examples_to_save:\n",
        "                to_save = min(num_examples_to_save - num_saved, batch_size)\n",
        "                for j in range(to_save):\n",
        "                    idx = i * batch_size + j\n",
        "                    degraded_path, _ = test_pairs[idx]\n",
        "                    base_name = os.path.splitext(os.path.basename(degraded_path))[0]\n",
        "                    _, input_phase = process_file_to_tensor(degraded_path, device)\n",
        "                    output_waveform = postprocess_audio(outputs[j].unsqueeze(0).cpu(), input_phase)\n",
        "                    output_filename = os.path.join(\n",
        "                        output_dir, f\"test_output_example_{num_saved+1}_{base_name}.wav\"\n",
        "                    )\n",
        "                    torchaudio.save(output_filename, output_waveform.cpu(), sample_rate)\n",
        "                    saved_filenames.append(output_filename)\n",
        "                    num_saved += 1\n",
        "\n",
        "    avg_loss = test_loss / num_examples if num_examples > 0 else float('inf')\n",
        "    print(f\"Test Loss (MSE) on {num_examples} test examples: {avg_loss:.4f}\")\n",
        "    print(f\"{num_examples_to_save} test output examples saved:\")\n",
        "    for fname in saved_filenames:\n",
        "        print(\"-\", fname)\n",
        "\n",
        "    return avg_loss, saved_filenames"
      ],
      "metadata": {
        "id": "cLKv-K8MT07a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}